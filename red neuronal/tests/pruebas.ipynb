{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c987d337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.65572464\n",
      "Validation score: 0.381162\n",
      "Iteration 2, loss = 1.58238754\n",
      "Validation score: 0.465105\n",
      "Iteration 3, loss = 0.76546856\n",
      "Validation score: 0.496828\n",
      "Iteration 4, loss = 0.41479810\n",
      "Validation score: 0.503660\n",
      "Iteration 5, loss = 0.26747060\n",
      "Validation score: 0.506101\n",
      "Iteration 6, loss = 0.18934325\n",
      "Validation score: 0.516837\n",
      "Iteration 7, loss = 0.15070470\n",
      "Validation score: 0.503172\n",
      "Iteration 8, loss = 0.13184505\n",
      "Validation score: 0.523182\n",
      "Iteration 9, loss = 0.12327368\n",
      "Validation score: 0.524158\n",
      "Iteration 10, loss = 0.11390210\n",
      "Validation score: 0.512445\n",
      "Iteration 11, loss = 0.10539486\n",
      "Validation score: 0.525134\n",
      "Iteration 12, loss = 0.10114332\n",
      "Validation score: 0.517814\n",
      "Iteration 13, loss = 0.09537005\n",
      "Validation score: 0.518302\n",
      "Iteration 14, loss = 0.09207052\n",
      "Validation score: 0.519278\n",
      "Iteration 15, loss = 0.09958905\n",
      "Validation score: 0.532943\n",
      "Iteration 16, loss = 0.08304752\n",
      "Validation score: 0.535383\n",
      "Iteration 17, loss = 0.06142364\n",
      "Validation score: 0.545144\n",
      "Iteration 18, loss = 0.06744043\n",
      "Validation score: 0.536847\n",
      "Iteration 19, loss = 0.05565833\n",
      "Validation score: 0.535383\n",
      "Iteration 20, loss = 0.07228796\n",
      "Validation score: 0.535871\n",
      "Iteration 21, loss = 0.08686263\n",
      "Validation score: 0.533919\n",
      "Iteration 22, loss = 0.09189489\n",
      "Validation score: 0.529039\n",
      "Iteration 23, loss = 0.08290955\n",
      "Validation score: 0.529039\n",
      "Iteration 24, loss = 0.06854366\n",
      "Validation score: 0.536847\n",
      "Iteration 25, loss = 0.06465468\n",
      "Validation score: 0.538799\n",
      "Iteration 26, loss = 0.06503874\n",
      "Validation score: 0.543192\n",
      "Iteration 27, loss = 0.07079064\n",
      "Validation score: 0.551489\n",
      "Iteration 28, loss = 0.06962664\n",
      "Validation score: 0.554905\n",
      "Iteration 29, loss = 0.09424326\n",
      "Validation score: 0.533431\n",
      "Iteration 30, loss = 0.12239400\n",
      "Validation score: 0.543680\n",
      "Iteration 31, loss = 0.09324852\n",
      "Validation score: 0.529527\n",
      "Iteration 32, loss = 0.07451194\n",
      "Validation score: 0.543192\n",
      "Iteration 33, loss = 0.05409777\n",
      "Validation score: 0.538311\n",
      "Iteration 34, loss = 0.04060296\n",
      "Validation score: 0.567106\n",
      "Iteration 35, loss = 0.03785924\n",
      "Validation score: 0.569058\n",
      "Iteration 36, loss = 0.06028871\n",
      "Validation score: 0.525134\n",
      "Iteration 37, loss = 0.12666410\n",
      "Validation score: 0.524646\n",
      "Iteration 38, loss = 0.13586155\n",
      "Validation score: 0.543192\n",
      "Iteration 39, loss = 0.11127627\n",
      "Validation score: 0.550024\n",
      "Iteration 40, loss = 0.09830084\n",
      "Validation score: 0.557345\n",
      "Iteration 41, loss = 0.10233335\n",
      "Validation score: 0.537823\n",
      "Iteration 42, loss = 0.08032203\n",
      "Validation score: 0.549048\n",
      "Iteration 43, loss = 0.06960321\n",
      "Validation score: 0.542704\n",
      "Iteration 44, loss = 0.06096473\n",
      "Validation score: 0.558809\n",
      "Iteration 45, loss = 0.07956525\n",
      "Validation score: 0.545632\n",
      "Iteration 46, loss = 0.10753569\n",
      "Validation score: 0.552465\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "🎯 Accuracy: 0.5562\n",
      "\n",
      "📋 Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      104101       0.81      0.68      0.74        19\n",
      "      104102       0.59      0.78      0.67        37\n",
      "      104103       0.86      0.62      0.72        29\n",
      "      104112       1.00      1.00      1.00         5\n",
      "      104131       0.38      0.56      0.45         9\n",
      "      104132       0.38      0.27      0.32        11\n",
      "      104133       0.00      0.00      0.00         6\n",
      "       11013       0.14      0.50      0.22         4\n",
      "       11101       0.09      0.06      0.07        17\n",
      "      111012       0.91      0.91      0.91        11\n",
      "       11102       0.14      0.19      0.16        16\n",
      "       11103       0.22      0.11      0.14        19\n",
      "      111201       0.06      0.07      0.06        15\n",
      "      111202       0.18      0.21      0.19        14\n",
      "      111203       0.13      0.15      0.14        13\n",
      "      111401       0.36      0.30      0.33        30\n",
      "      111402       0.14      0.16      0.15        19\n",
      "      111403       0.17      0.11      0.13         9\n",
      "       11201       0.14      0.20      0.17         5\n",
      "      114101       0.89      0.62      0.73        13\n",
      "      114102       0.84      0.72      0.78        58\n",
      "      114103       0.71      0.79      0.75        38\n",
      "      114111       0.71      0.83      0.77         6\n",
      "      114112       0.40      0.22      0.29         9\n",
      "      114113       0.50      0.67      0.57         6\n",
      "      114121       0.79      0.78      0.78        49\n",
      "      114122       0.85      0.78      0.82       138\n",
      "      114123       0.65      0.72      0.68        87\n",
      "      114131       0.66      0.75      0.70        57\n",
      "      114132       0.62      0.52      0.57        29\n",
      "      114133       0.45      0.69      0.55        13\n",
      "      114142       0.71      0.77      0.74        13\n",
      "      114143       0.14      0.20      0.17         5\n",
      "      114161       0.30      0.33      0.32         9\n",
      "      114162       0.53      0.50      0.51        18\n",
      "      114163       0.50      0.40      0.44        15\n",
      "      121101       0.43      0.27      0.33        11\n",
      "      121102       0.08      0.08      0.08        13\n",
      "      121103       0.33      0.17      0.22         6\n",
      "      121132       0.44      0.67      0.53         6\n",
      "      121141       0.29      0.33      0.31         6\n",
      "      121142       0.11      0.12      0.12         8\n",
      "      124111       0.57      0.62      0.59        13\n",
      "      124112       0.42      0.57      0.48        14\n",
      "      124113       0.25      0.20      0.22         5\n",
      "      131201       0.25      0.14      0.18         7\n",
      "      134121       0.50      0.60      0.55         5\n",
      "       14111       0.20      0.17      0.18         6\n",
      "       14112       0.36      0.42      0.38        12\n",
      "       14113       0.44      0.64      0.52        11\n",
      "       14132       0.60      0.38      0.46         8\n",
      "       14133       0.50      0.58      0.54        12\n",
      "       14142       0.75      0.50      0.60         6\n",
      "       14143       0.67      0.91      0.77        11\n",
      "       14152       1.00      0.60      0.75         5\n",
      "       14153       0.62      0.56      0.59         9\n",
      "       14162       0.83      0.83      0.83        12\n",
      "       14163       0.70      0.78      0.74         9\n",
      "       14171       0.17      0.14      0.15         7\n",
      "       14172       0.50      0.62      0.56         8\n",
      "       14212       0.33      0.12      0.18         8\n",
      "       14213       0.44      0.57      0.50         7\n",
      "       14221       0.92      0.92      0.92        73\n",
      "       14222       0.87      0.57      0.69        35\n",
      "       14223       0.48      0.71      0.57        17\n",
      "       14231       0.83      0.81      0.82        31\n",
      "       14232       0.64      0.91      0.75        23\n",
      "       14233       0.33      0.22      0.27         9\n",
      "       14241       0.47      0.86      0.61        21\n",
      "       14242       0.50      0.43      0.46        21\n",
      "       14243       0.33      0.43      0.38        14\n",
      "       14261       0.22      0.20      0.21        10\n",
      "       14262       0.53      0.62      0.57        32\n",
      "       14263       0.67      0.71      0.69        48\n",
      "      161201       0.25      0.17      0.20         6\n",
      "      161202       0.00      0.00      0.00         7\n",
      "      161203       0.67      0.17      0.27        12\n",
      "      164111       0.93      0.87      0.90        15\n",
      "      164112       0.86      0.60      0.71        10\n",
      "      164113       0.40      0.29      0.33         7\n",
      "      164131       0.44      0.57      0.50         7\n",
      "      164132       0.33      0.15      0.21        13\n",
      "      164133       0.43      0.75      0.55         8\n",
      "      164141       0.83      1.00      0.91         5\n",
      "      174103       1.00      1.00      1.00         8\n",
      "      174111       0.40      0.40      0.40         5\n",
      "      174112       0.33      0.38      0.35         8\n",
      "      174113       0.82      0.50      0.62        18\n",
      "      174132       0.97      0.93      0.95        40\n",
      "      174133       0.96      0.91      0.94        57\n",
      "      174143       0.75      0.50      0.60         6\n",
      "      184111       0.30      0.78      0.44         9\n",
      "      184112       0.00      0.00      0.00        11\n",
      "      184113       0.50      0.17      0.25         6\n",
      "      184131       0.73      0.57      0.64        14\n",
      "      184132       0.60      0.43      0.50         7\n",
      "      184133       0.83      0.83      0.83        24\n",
      "      184141       0.50      0.29      0.36         7\n",
      "      184142       0.00      0.00      0.00         4\n",
      "      184143       0.75      0.33      0.46         9\n",
      "      184151       0.50      0.60      0.55         5\n",
      "      184152       0.31      0.44      0.36         9\n",
      "      184153       0.47      0.58      0.52        12\n",
      "      194162       0.83      1.00      0.91         5\n",
      "       21201       0.25      0.25      0.25         4\n",
      "       21202       0.00      0.00      0.00         5\n",
      "      214101       0.80      1.00      0.89         4\n",
      "      214121       0.25      0.25      0.25        12\n",
      "      214122       0.22      0.44      0.30        25\n",
      "      214123       0.50      0.53      0.52        30\n",
      "      214131       0.71      0.56      0.62         9\n",
      "      214132       0.55      0.73      0.63        15\n",
      "      214133       0.43      0.33      0.38         9\n",
      "       24121       1.00      0.40      0.57         5\n",
      "       31201       0.00      0.00      0.00        21\n",
      "       31202       0.15      0.20      0.17        15\n",
      "       31203       0.15      0.14      0.15        28\n",
      "       34102       0.12      0.20      0.15         5\n",
      "       34111       0.80      0.50      0.62         8\n",
      "       34112       0.58      0.78      0.67         9\n",
      "       34113       0.67      0.57      0.62         7\n",
      "       34131       0.90      0.76      0.83        25\n",
      "       34132       0.40      0.22      0.29         9\n",
      "       34133       0.17      0.25      0.20         4\n",
      "       34141       0.80      0.80      0.80         5\n",
      "       34151       0.60      0.46      0.52        13\n",
      "       34152       0.22      0.25      0.24         8\n",
      "       41013       0.00      0.00      0.00         8\n",
      "       41023       0.15      0.24      0.19        17\n",
      "       41103       0.20      0.09      0.12        11\n",
      "       41202       0.00      0.00      0.00         6\n",
      "       41203       0.09      0.08      0.09        25\n",
      "       41302       0.00      0.00      0.00         4\n",
      "       41303       0.00      0.00      0.00         9\n",
      "       44123       0.83      0.71      0.77         7\n",
      "       44142       0.80      0.67      0.73        12\n",
      "       44143       0.69      0.85      0.76        26\n",
      "       44151       0.67      0.29      0.40         7\n",
      "       44152       0.71      0.58      0.64        26\n",
      "       44153       0.67      0.68      0.67        41\n",
      "       44161       0.86      0.88      0.87        41\n",
      "       44162       0.82      0.72      0.77        71\n",
      "       44163       0.79      0.85      0.82       116\n",
      "       44171       0.79      0.68      0.73        34\n",
      "       44172       0.70      0.81      0.75        53\n",
      "       44173       0.92      0.89      0.90       146\n",
      "       44181       0.85      0.95      0.90        41\n",
      "       44182       0.67      0.40      0.50        10\n",
      "       44183       0.20      0.14      0.17         7\n",
      "       44193       1.00      1.00      1.00         4\n",
      "       44201       0.80      0.80      0.80         5\n",
      "       44202       0.71      0.62      0.67         8\n",
      "       44203       0.38      0.45      0.42        11\n",
      "       44211       0.57      0.31      0.40        13\n",
      "       44212       0.47      0.37      0.41        19\n",
      "       44213       0.49      0.63      0.55        27\n",
      "       51053       0.20      0.40      0.27         5\n",
      "       51063       0.00      0.00      0.00         4\n",
      "       51122       0.00      0.00      0.00         6\n",
      "       51123       0.07      0.08      0.07        12\n",
      "       51151       0.00      0.00      0.00         4\n",
      "       51152       0.40      0.17      0.24        12\n",
      "       51153       0.12      0.15      0.13        34\n",
      "       51163       0.00      0.00      0.00         6\n",
      "       51201       0.00      0.00      0.00         6\n",
      "       51202       0.21      0.29      0.24        21\n",
      "       51203       0.09      0.08      0.08        26\n",
      "       51402       0.20      0.17      0.18         6\n",
      "       51403       0.22      0.24      0.23        17\n",
      "       54102       0.40      0.50      0.44         4\n",
      "       54103       0.27      0.27      0.27        15\n",
      "       54121       1.00      0.56      0.71         9\n",
      "       54122       0.65      0.74      0.69        50\n",
      "       54123       0.84      0.78      0.81       126\n",
      "       54131       0.88      0.88      0.88         8\n",
      "       54132       0.62      0.71      0.67         7\n",
      "       54133       0.58      0.50      0.54        14\n",
      "       54151       0.65      0.60      0.62        25\n",
      "       54152       0.57      0.58      0.57        45\n",
      "       54153       0.57      0.57      0.57        42\n",
      "       54163       0.25      0.25      0.25         4\n",
      "       54171       1.00      0.50      0.67         4\n",
      "       54172       0.77      0.83      0.80        12\n",
      "       54173       1.00      0.83      0.91         6\n",
      "       54182       1.00      0.20      0.33         5\n",
      "       54183       0.50      0.43      0.46         7\n",
      "       54191       0.50      0.38      0.43         8\n",
      "       54192       0.38      0.33      0.36        15\n",
      "       54193       0.87      0.81      0.84        32\n",
      "       54201       0.71      0.86      0.77        14\n",
      "       54202       0.72      0.54      0.62        24\n",
      "       54203       0.77      0.45      0.57        22\n",
      "       54212       0.88      1.00      0.93         7\n",
      "       54232       0.62      0.68      0.65        19\n",
      "       54233       0.67      0.55      0.60        11\n",
      "       54241       0.00      0.00      0.00         4\n",
      "       54242       0.78      0.88      0.82         8\n",
      "       54243       0.71      0.62      0.67        16\n",
      "       61103       0.14      0.14      0.14         7\n",
      "       61111       0.16      0.60      0.25         5\n",
      "       61112       0.12      0.17      0.14         6\n",
      "       61113       0.00      0.00      0.00         5\n",
      "       61123       0.25      0.25      0.25         4\n",
      "       61131       0.96      0.81      0.88        32\n",
      "       61132       0.29      0.33      0.31         6\n",
      "       61133       0.25      0.33      0.29         6\n",
      "       61202       0.11      0.09      0.10        11\n",
      "       61203       0.20      0.13      0.16        23\n",
      "       61303       0.00      0.00      0.00         4\n",
      "       61403       0.00      0.00      0.00         5\n",
      "       64101       0.60      0.60      0.60         5\n",
      "       64102       0.64      0.64      0.64        14\n",
      "       64103       0.54      0.54      0.54        24\n",
      "       64111       0.80      0.91      0.85        22\n",
      "       64112       0.75      0.82      0.78        22\n",
      "       64113       1.00      0.47      0.64        15\n",
      "       64121       0.78      0.64      0.70        11\n",
      "       64122       0.60      0.69      0.64        13\n",
      "       64123       0.58      0.61      0.59        18\n",
      "       64131       0.86      0.80      0.83        15\n",
      "       64132       0.75      0.43      0.55        14\n",
      "       64133       0.63      0.80      0.71        15\n",
      "       64151       0.83      1.00      0.91         5\n",
      "       64153       0.25      0.20      0.22         5\n",
      "       64162       0.50      0.80      0.62         5\n",
      "       64163       0.67      0.40      0.50         5\n",
      "       64171       0.87      0.94      0.91        36\n",
      "       64172       0.83      0.76      0.79        33\n",
      "       64173       0.88      0.54      0.67        26\n",
      "       64181       0.66      0.68      0.67        40\n",
      "       64182       0.54      0.44      0.48        32\n",
      "       64183       0.62      0.53      0.57        30\n",
      "       71102       0.00      0.00      0.00         8\n",
      "       71103       0.00      0.00      0.00        12\n",
      "       71121       0.00      0.00      0.00         4\n",
      "       71122       0.00      0.00      0.00         5\n",
      "       71123       0.00      0.00      0.00         8\n",
      "       71141       0.76      0.82      0.79        34\n",
      "       71142       0.36      0.36      0.36        25\n",
      "       71143       0.42      0.38      0.40        21\n",
      "       74102       0.50      0.25      0.33         8\n",
      "       74103       0.80      0.90      0.85        31\n",
      "       74111       0.62      0.71      0.67         7\n",
      "       74112       0.62      0.62      0.62        13\n",
      "       74113       0.83      0.59      0.69        32\n",
      "       74122       1.00      0.33      0.50         6\n",
      "       74123       0.60      0.75      0.67         8\n",
      "       74131       0.67      0.67      0.67        12\n",
      "       74132       0.56      0.75      0.64        20\n",
      "       74133       0.76      0.62      0.68        21\n",
      "       74141       0.64      0.60      0.62        15\n",
      "       74142       0.47      0.61      0.53        23\n",
      "       74143       0.48      0.43      0.45        23\n",
      "       81022       0.20      0.07      0.11        14\n",
      "       81023       1.00      0.17      0.29        12\n",
      "       81042       0.21      0.60      0.32         5\n",
      "       81071       0.21      0.27      0.24        11\n",
      "       81072       0.17      0.33      0.23        12\n",
      "       81073       0.00      0.00      0.00         6\n",
      "       81201       0.34      0.34      0.34        32\n",
      "       81202       0.62      0.19      0.29        27\n",
      "       81203       0.40      0.42      0.41        24\n",
      "       81301       0.00      0.00      0.00         8\n",
      "       81302       0.25      0.25      0.25         8\n",
      "       81303       0.00      0.00      0.00         4\n",
      "       81501       0.00      0.00      0.00         6\n",
      "       81601       0.18      0.21      0.20        33\n",
      "       81602       0.10      0.17      0.12        24\n",
      "       81603       0.06      0.06      0.06        16\n",
      "       81701       0.38      0.46      0.41        37\n",
      "       81702       0.09      0.07      0.08        14\n",
      "       81703       0.04      0.12      0.06         8\n",
      "       81801       0.58      0.48      0.53        52\n",
      "       81802       0.19      0.21      0.20        14\n",
      "       81803       0.20      0.25      0.22         8\n",
      "       84121       0.25      0.41      0.31        17\n",
      "       84122       0.00      0.00      0.00         7\n",
      "       84161       0.57      0.57      0.57         7\n",
      "       84162       0.14      0.10      0.12        10\n",
      "       84163       0.75      0.60      0.67        15\n",
      "       84171       0.61      0.65      0.63        17\n",
      "       84172       0.67      0.18      0.29        11\n",
      "       84173       0.00      0.00      0.00         6\n",
      "       84181       0.57      0.80      0.67         5\n",
      "       84182       0.00      0.00      0.00         5\n",
      "       84201       0.83      1.00      0.91         5\n",
      "       84211       0.29      0.46      0.35        13\n",
      "       84212       0.20      0.12      0.15         8\n",
      "       84213       0.00      0.00      0.00         4\n",
      "       91301       0.24      0.28      0.25        29\n",
      "       91302       0.00      0.00      0.00        13\n",
      "       91303       0.38      0.35      0.36        17\n",
      "       91401       0.17      0.20      0.18         5\n",
      "       91501       0.40      0.50      0.44         4\n",
      "       94102       0.31      0.62      0.42         8\n",
      "       94103       0.00      0.00      0.00         4\n",
      "       94121       0.69      0.64      0.67        28\n",
      "       94122       0.53      0.38      0.44        21\n",
      "       94123       0.53      0.83      0.65        12\n",
      "       94131       0.32      0.46      0.38        13\n",
      "       94132       0.00      0.00      0.00         7\n",
      "       94133       0.00      0.00      0.00         5\n",
      "       94141       0.60      0.64      0.62        14\n",
      "       94142       0.50      0.50      0.50        10\n",
      "       94143       0.25      0.17      0.20         6\n",
      "\n",
      "    accuracy                           0.56      5122\n",
      "   macro avg       0.46      0.44      0.44      5122\n",
      "weighted avg       0.57      0.56      0.56      5122\n",
      "\n",
      "🔢 Matriz de confusión:\n",
      "[[13  5  0 ...  0  0  0]\n",
      " [ 1 29  0 ...  0  0  0]\n",
      " [ 1  8 18 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  9  3  1]\n",
      " [ 0  0  0 ...  3  5  0]\n",
      " [ 0  0  0 ...  0  1  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\esteb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\esteb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1️⃣ Cargar datos ya prefiltrados manualmente\n",
    "df = pd.read_excel(\"base_limpia_grd_min20.xlsx\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "df.fillna('Desconocido', inplace=True)\n",
    "\n",
    "# 2️⃣ Features (verifica que estas columnas existan en tu archivo Excel)\n",
    "features = [\n",
    "    'Edad', 'Grupo Edad', 'Sexo', 'Codigo de ciudad', 'Tipo de ingreso',\n",
    "    'Días estancia', 'ServicioAlta', 'Cuidados intensivos', 'Días de Unidad Cuidado Intensivo',\n",
    "    'Dx de ingreso', 'Dx principal de egreso', 'Dx principal de egreso .1', 'Dx Ppal 3 Caracteres',\n",
    "    'Dxr 1', 'Dxr 2', 'Dxr 3', 'Dxr 4', 'Dxr 5',\n",
    "    'Código causa externa', 'Causa externa', 'Situacion al alta',\n",
    "    'Proc1', 'Proc2', 'Proc3', 'Proc4', 'Proc5', 'Proc6', 'Proc7', 'Proc8', 'Proc9', 'Proc10',\n",
    "    'Tipo servicio', 'Causa Basica de muerte', 'Infecciones', 'Infección Quirurgica'\n",
    "]\n",
    "\n",
    "# Filtrar las features existentes en la base\n",
    "features = [f for f in features if f in df.columns]\n",
    "\n",
    "# 3️⃣ Matriz de entrada y etiquetas\n",
    "X = pd.get_dummies(df[features], drop_first=True)\n",
    "y = df['GRD -Código'].astype(str)\n",
    "\n",
    "# 4️⃣ Dividir antes de codificar etiquetas\n",
    "X_train, X_test, y_train_raw, y_test_raw = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 5️⃣ Contar frecuencia total de GRD en todo el dataset\n",
    "y_counts = y.value_counts()\n",
    "valid_grds = y_counts[y_counts >= 20].index\n",
    "\n",
    "# 6️⃣ Filtrar conjuntos de entrenamiento y prueba con solo GRD válidos\n",
    "mask_train = y_train_raw.isin(valid_grds)\n",
    "mask_test = y_test_raw.isin(valid_grds)\n",
    "\n",
    "X_train = X_train[mask_train]\n",
    "y_train_raw = y_train_raw[mask_train]\n",
    "\n",
    "X_test = X_test[mask_test]\n",
    "y_test_raw = y_test_raw[mask_test]\n",
    "\n",
    "# 7️⃣ Codificar etiquetas\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_raw)\n",
    "y_test = le.transform(y_test_raw)\n",
    "\n",
    "# 8️⃣ Escalado\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# 9️⃣ Entrenamiento con MLPClassifier\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=0.001,\n",
    "    batch_size=32,\n",
    "    max_iter=100,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "mlp.fit(X_train_s, y_train)\n",
    "\n",
    "# 🔟 Evaluación\n",
    "y_pred = mlp.predict(X_test_s)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n🎯 Accuracy: {acc:.4f}\\n\")\n",
    "print(\"📋 Reporte de clasificación:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.inverse_transform(np.unique(y_test))))\n",
    "print(\"🔢 Matriz de confusión:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bec8ca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
