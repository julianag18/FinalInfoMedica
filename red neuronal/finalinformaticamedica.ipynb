{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e3d8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas originales: 68\n",
      "Columnas conservadas (>= 2% datos no nulos): 49\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"dabe de datos.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Umbral: conservar solo columnas con al menos el 10% de datos no nulos\n",
    "min_valid_ratio = 0.02\n",
    "min_valid_count = int(len(df) * min_valid_ratio)\n",
    "\n",
    "cols_to_keep = [col for col in df.columns if df[col].notna().sum() >= min_valid_count]\n",
    "df_cleaned = df[cols_to_keep]\n",
    "\n",
    "df_cleaned.to_excel(\"datos_limpios.xlsx\", index=False)\n",
    "\n",
    "print(f\"Columnas originales: {len(df.columns)}\")\n",
    "print(f\"Columnas conservadas (>= {min_valid_ratio*100:.0f}% datos no nulos): {len(df_cleaned.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dea32ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.60865625\n",
      "Validation score: 0.737220\n",
      "Iteration 2, loss = 0.50468571\n",
      "Validation score: 0.760987\n",
      "Iteration 3, loss = 0.21295736\n",
      "Validation score: 0.756951\n",
      "Iteration 4, loss = 0.12213428\n",
      "Validation score: 0.755157\n",
      "Iteration 5, loss = 0.08079491\n",
      "Validation score: 0.768161\n",
      "Iteration 6, loss = 0.06310853\n",
      "Validation score: 0.763677\n",
      "Iteration 7, loss = 0.05638753\n",
      "Validation score: 0.765471\n",
      "Iteration 8, loss = 0.05627546\n",
      "Validation score: 0.769955\n",
      "Iteration 9, loss = 0.05541248\n",
      "Validation score: 0.757848\n",
      "Iteration 10, loss = 0.06407508\n",
      "Validation score: 0.753363\n",
      "Iteration 11, loss = 0.08584341\n",
      "Validation score: 0.752466\n",
      "Iteration 12, loss = 0.07837976\n",
      "Validation score: 0.770852\n",
      "Iteration 13, loss = 0.06517766\n",
      "Validation score: 0.765919\n",
      "Iteration 14, loss = 0.06007152\n",
      "Validation score: 0.763677\n",
      "Iteration 15, loss = 0.05510257\n",
      "Validation score: 0.777130\n",
      "Iteration 16, loss = 0.05482527\n",
      "Validation score: 0.765919\n",
      "Iteration 17, loss = 0.06282414\n",
      "Validation score: 0.778027\n",
      "Iteration 18, loss = 0.06619412\n",
      "Validation score: 0.772197\n",
      "Iteration 19, loss = 0.07446094\n",
      "Validation score: 0.781166\n",
      "Iteration 20, loss = 0.07082237\n",
      "Validation score: 0.796413\n",
      "Iteration 21, loss = 0.07246133\n",
      "Validation score: 0.775785\n",
      "Iteration 22, loss = 0.06982244\n",
      "Validation score: 0.786996\n",
      "Iteration 23, loss = 0.06683532\n",
      "Validation score: 0.793722\n",
      "Iteration 24, loss = 0.06259705\n",
      "Validation score: 0.795067\n",
      "Iteration 25, loss = 0.05801029\n",
      "Validation score: 0.796861\n",
      "Iteration 26, loss = 0.05677484\n",
      "Validation score: 0.795516\n",
      "Iteration 27, loss = 0.05813487\n",
      "Validation score: 0.800897\n",
      "Iteration 28, loss = 0.06530153\n",
      "Validation score: 0.799552\n",
      "Iteration 29, loss = 0.06698502\n",
      "Validation score: 0.793722\n",
      "Iteration 30, loss = 0.05649405\n",
      "Validation score: 0.791928\n",
      "Iteration 31, loss = 0.05706994\n",
      "Validation score: 0.794170\n",
      "Iteration 32, loss = 0.06153081\n",
      "Validation score: 0.788341\n",
      "Iteration 33, loss = 0.07244565\n",
      "Validation score: 0.795067\n",
      "Iteration 34, loss = 0.06424734\n",
      "Validation score: 0.798655\n",
      "Iteration 35, loss = 0.06430767\n",
      "Validation score: 0.777578\n",
      "Iteration 36, loss = 0.06641345\n",
      "Validation score: 0.782511\n",
      "Iteration 37, loss = 0.06541894\n",
      "Validation score: 0.786547\n",
      "Iteration 38, loss = 0.06861483\n",
      "Validation score: 0.795964\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "üìã Reporte de clasificaci√≥n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      104102       0.62      0.65      0.63        37\n",
      "      104103       0.82      0.48      0.61        29\n",
      "      111401       0.47      0.53      0.50        30\n",
      "      114102       0.71      0.59      0.64        58\n",
      "      114103       0.61      0.50      0.55        38\n",
      "      114121       0.68      0.61      0.65        49\n",
      "      114122       0.79      0.72      0.75       138\n",
      "      114123       0.75      0.62      0.68        87\n",
      "      114131       0.73      0.67      0.70        57\n",
      "      114132       0.62      0.45      0.52        29\n",
      "       14221       0.88      0.90      0.89        73\n",
      "       14222       0.84      0.60      0.70        35\n",
      "       14231       0.77      0.77      0.77        31\n",
      "       14262       0.35      0.62      0.45        32\n",
      "       14263       0.75      0.50      0.60        48\n",
      "      174132       1.00      0.88      0.93        40\n",
      "      174133       0.95      1.00      0.97        57\n",
      "      214123       0.40      0.33      0.36        30\n",
      "       44153       0.52      0.39      0.44        41\n",
      "       44161       0.85      0.83      0.84        41\n",
      "       44162       0.74      0.75      0.74        71\n",
      "       44163       0.79      0.63      0.70       116\n",
      "       44171       0.55      0.76      0.64        34\n",
      "       44172       0.61      0.58      0.60        53\n",
      "       44173       0.89      0.77      0.83       146\n",
      "       44181       0.88      0.90      0.89        41\n",
      "       51153       0.14      0.03      0.05        34\n",
      "       54122       0.62      0.52      0.57        50\n",
      "       54123       0.73      0.72      0.73       126\n",
      "       54152       0.48      0.47      0.47        45\n",
      "       54153       0.67      0.14      0.24        42\n",
      "       54193       0.89      0.53      0.67        32\n",
      "       61131       0.88      0.91      0.89        32\n",
      "       64171       0.96      0.64      0.77        36\n",
      "       64172       0.65      0.91      0.76        33\n",
      "       64181       0.53      0.60      0.56        40\n",
      "       64182       0.25      0.19      0.21        32\n",
      "       64183       0.55      0.40      0.46        30\n",
      "       71141       0.77      0.68      0.72        34\n",
      "       74103       0.95      0.58      0.72        31\n",
      "       74113       0.68      0.66      0.67        32\n",
      "       81201       0.44      0.56      0.49        32\n",
      "       81601       0.20      0.24      0.22        33\n",
      "       81701       0.67      0.32      0.44        37\n",
      "       81801       0.48      0.48      0.48        52\n",
      "        OTRO       0.85      0.91      0.88      3350\n",
      "\n",
      "    accuracy                           0.80      5574\n",
      "   macro avg       0.67      0.60      0.62      5574\n",
      "weighted avg       0.79      0.80      0.79      5574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1Ô∏è‚É£ Cargar datos\n",
    "df = pd.read_excel(\"datos_limpios.xlsx\")\n",
    "\n",
    "# 2Ô∏è‚É£ Imputaci√≥n de valores nulos\n",
    "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "df.fillna('Desconocido', inplace=True)\n",
    "\n",
    "# 3Ô∏è‚É£ Definir variables predictoras (basadas en las columnas conservadas)\n",
    "features = [\n",
    "    'Edad',\n",
    "    'Sexo', 'Tipo de ingreso', 'Dx principal de egreso',\n",
    "    'D√≠as estancia', 'ServicioAlta', 'D√≠as de Unidad Cuidado Intensivo',\n",
    "    'Dxr 1', 'Dxr 2', 'Dxr 3', 'Dxr 4', 'Dxr 5', 'Dxr-6', 'Dxr 7', 'Dxr 8', 'Dxr 9',\n",
    "    'C√≥digo causa externa',  'Situacion al alta',\n",
    "    'Proc1', 'Proc2', 'Proc3', 'Proc4', 'Proc5', 'Proc6',\n",
    "    'Proc7', 'Proc8', 'Proc9', 'Proc10', 'Proc11', 'Proc12',\n",
    "    'Tipo servicio', 'Infecciones'\n",
    "]\n",
    "\n",
    "X = pd.get_dummies(df[features])\n",
    "\n",
    "# 4Ô∏è‚É£ Preparar etiquetas (GRD - C√≥digo)\n",
    "y_raw = df['GRD -C√≥digo'].astype(str)\n",
    "\n",
    "# Filtrar GRDs frecuentes (‚â• 20% del GRD m√°s frecuente)\n",
    "grd_counts = y_raw.value_counts()\n",
    "umbral = grd_counts.max() * 0.20\n",
    "grds_frec = grd_counts[grd_counts >= umbral].index\n",
    "y_filtered = y_raw.where(y_raw.isin(grds_frec), 'OTRO')\n",
    "\n",
    "# Codificar etiquetas\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_filtered)\n",
    "\n",
    "# Eliminar clases con solo 1 muestra\n",
    "mask = [Counter(y_encoded)[label] > 1 for label in y_encoded]\n",
    "X = X[mask]\n",
    "y_encoded = y_encoded[mask]\n",
    "\n",
    "# 5Ô∏è‚É£ Dividir conjunto de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# 6Ô∏è‚É£ Escalado\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 7Ô∏è‚É£ Definir modelo\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64, 32),\n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    alpha=0.001,\n",
    "                    batch_size=32,\n",
    "                    learning_rate='adaptive',\n",
    "                    max_iter=100,\n",
    "                    early_stopping=True,\n",
    "                    validation_fraction=0.1,\n",
    "                    n_iter_no_change=10,\n",
    "                    random_state=42,\n",
    "                    verbose=True)\n",
    "\n",
    "# 8Ô∏è‚É£ Entrenar modelo\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# 9Ô∏è‚É£ Evaluaci√≥n\n",
    "y_pred = mlp.predict(X_test)\n",
    "y_pred_proba = mlp.predict_proba(X_test)\n",
    "\n",
    "print(\"\\nüìã Reporte de clasificaci√≥n:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# # üîØ Matriz de confusi√≥n\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "#             xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "# plt.title(\"Matriz de Confusi√≥n - MLPClassifier (Model_2)\")\n",
    "# plt.xlabel(\"Predicci√≥n\")\n",
    "# plt.ylabel(\"Etiqueta Real\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # üîØ Curvas ROC por clase\n",
    "# y_test_bin = label_binarize(y_test, classes=np.arange(len(le.classes_)))\n",
    "# plt.figure(figsize=(8,6))\n",
    "# for i in range(len(le.classes_)):\n",
    "#     fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     plt.plot(fpr, tpr, label=f'{le.classes_[i]} (AUC = {roc_auc:.2f})')\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.title('Curvas ROC por clase - MLPClassifier')\n",
    "# plt.xlabel('Tasa de Falsos Positivos')\n",
    "# plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b39a68e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.81973398\n",
      "Validation score: 0.691011\n",
      "Iteration 2, loss = 0.29584614\n",
      "Validation score: 0.719101\n",
      "Iteration 3, loss = 0.11079354\n",
      "Validation score: 0.734831\n",
      "Iteration 4, loss = 0.06410279\n",
      "Validation score: 0.752809\n",
      "Iteration 5, loss = 0.04627133\n",
      "Validation score: 0.786517\n",
      "Iteration 6, loss = 0.03802257\n",
      "Validation score: 0.788764\n",
      "Iteration 7, loss = 0.03283653\n",
      "Validation score: 0.795506\n",
      "Iteration 8, loss = 0.03013335\n",
      "Validation score: 0.797753\n",
      "Iteration 9, loss = 0.02698330\n",
      "Validation score: 0.793258\n",
      "Iteration 10, loss = 0.03052640\n",
      "Validation score: 0.794382\n",
      "Iteration 11, loss = 0.02957596\n",
      "Validation score: 0.798876\n",
      "Iteration 12, loss = 0.02745518\n",
      "Validation score: 0.796629\n",
      "Iteration 13, loss = 0.02840545\n",
      "Validation score: 0.808989\n",
      "Iteration 14, loss = 0.03330989\n",
      "Validation score: 0.796629\n",
      "Iteration 15, loss = 0.34097177\n",
      "Validation score: 0.721348\n",
      "Iteration 16, loss = 0.33924197\n",
      "Validation score: 0.730337\n",
      "Iteration 17, loss = 0.14601229\n",
      "Validation score: 0.755056\n",
      "Iteration 18, loss = 0.08071210\n",
      "Validation score: 0.776404\n",
      "Iteration 19, loss = 0.05683462\n",
      "Validation score: 0.784270\n",
      "Iteration 20, loss = 0.05240792\n",
      "Validation score: 0.791011\n",
      "Iteration 21, loss = 0.05107274\n",
      "Validation score: 0.780899\n",
      "Iteration 22, loss = 0.04773493\n",
      "Validation score: 0.794382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Reporte de clasificaci√≥n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      104102       0.69      0.95      0.80        37\n",
      "      104103       0.94      0.52      0.67        29\n",
      "      111401       0.61      0.67      0.63        30\n",
      "      114102       0.84      0.93      0.89        58\n",
      "      114103       0.88      0.74      0.80        38\n",
      "      114121       0.65      0.63      0.64        49\n",
      "      114122       0.74      0.80      0.77       138\n",
      "      114123       0.78      0.70      0.74        87\n",
      "      114131       0.77      0.81      0.79        57\n",
      "      114132       0.85      0.79      0.82        29\n",
      "       14221       0.84      0.92      0.88        73\n",
      "       14222       0.77      0.69      0.73        35\n",
      "       14231       1.00      1.00      1.00        31\n",
      "       14262       0.69      0.69      0.69        32\n",
      "       14263       0.80      0.77      0.79        48\n",
      "      174132       0.98      1.00      0.99        40\n",
      "      174133       1.00      1.00      1.00        57\n",
      "      214123       0.94      0.97      0.95        30\n",
      "       44153       0.92      0.88      0.90        41\n",
      "       44161       0.84      0.76      0.79        41\n",
      "       44162       0.82      0.76      0.79        71\n",
      "       44163       0.81      0.90      0.85       116\n",
      "       44171       0.74      0.76      0.75        34\n",
      "       44172       0.71      0.55      0.62        53\n",
      "       44173       0.86      0.95      0.90       146\n",
      "       44181       0.98      1.00      0.99        41\n",
      "       51153       0.68      0.44      0.54        34\n",
      "       54122       0.73      0.64      0.68        50\n",
      "       54123       0.78      0.88      0.83       126\n",
      "       54152       0.74      0.69      0.71        45\n",
      "       54153       0.71      0.71      0.71        42\n",
      "       54193       0.93      0.81      0.87        32\n",
      "       61131       0.94      1.00      0.97        32\n",
      "       64171       0.91      0.81      0.85        36\n",
      "       64172       0.83      0.88      0.85        33\n",
      "       64181       0.66      0.62      0.64        40\n",
      "       64182       0.42      0.47      0.44        32\n",
      "       64183       0.52      0.47      0.49        30\n",
      "       71141       0.94      0.97      0.96        34\n",
      "       74103       1.00      0.97      0.98        31\n",
      "       74113       1.00      0.97      0.98        32\n",
      "       81201       0.85      0.72      0.78        32\n",
      "       81601       0.61      0.58      0.59        33\n",
      "       81701       0.79      0.81      0.80        37\n",
      "       81801       0.72      0.75      0.74        52\n",
      "\n",
      "    accuracy                           0.80      2224\n",
      "   macro avg       0.80      0.78      0.79      2224\n",
      "weighted avg       0.80      0.80      0.80      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1Ô∏è‚É£ Cargar datos\n",
    "df = pd.read_excel(\"datos_limpios.xlsx\")\n",
    "\n",
    "# 2Ô∏è‚É£ Imputaci√≥n de valores nulos\n",
    "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "df.fillna('Desconocido', inplace=True)\n",
    "\n",
    "# 3Ô∏è‚É£ Definir variables predictoras\n",
    "features = [\n",
    "    'Num caso', 'Aseguradora -C√≥digo-', 'Aseguradora -Descripci√≥n-', 'Edad', 'Grupo Edad',\n",
    "    'Sexo', 'Codigo de ciudad', 'Fecha de ingreso', 'Tipo de ingreso', 'Fecha de egreso',\n",
    "    'D√≠as estancia', 'ServicioAlta', 'Cuidados intensivos', 'D√≠as de Unidad Cuidado Intensivo',\n",
    "    'Dx de ingreso', 'Dx principal de egreso', 'Dx principal de egreso .1', 'Dx Ppal 3 Caracteres',\n",
    "    'Dxr 1', 'Dxr 2', 'Dxr 3', 'Dxr 4', 'Dxr 5', 'Dxr-6', 'Dxr 7', 'Dxr 8', 'Dxr 9',\n",
    "    'C√≥digo causa externa', 'Causa externa', 'Situacion al alta',\n",
    "    'Proc1', 'Proc2', 'Proc3', 'Proc4', 'Proc5', 'Proc6',\n",
    "    'Proc7', 'Proc8', 'Proc9', 'Proc10', 'Proc11', 'Proc12',\n",
    "    'Tipo servicio', 'Causa Basica de muerte', 'Infecciones', 'Infecci√≥n Quirurgica'\n",
    "]\n",
    "\n",
    "X = pd.get_dummies(df[features])\n",
    "\n",
    "# 4Ô∏è‚É£ Preparar etiquetas (GRD - C√≥digo)\n",
    "y_raw = df['GRD -C√≥digo'].astype(str)\n",
    "\n",
    "# Filtrar GRDs frecuentes (‚â• 20% del GRD m√°s frecuente)\n",
    "grds_counts = y_raw.value_counts()\n",
    "umbral = grds_counts.max() * 0.20\n",
    "grds_frec = grds_counts[grds_counts >= umbral].index\n",
    "\n",
    "# Eliminar filas con GRD poco frecuentes\n",
    "mask_frec = y_raw.isin(grds_frec)\n",
    "df = df[mask_frec]\n",
    "X = X[mask_frec]\n",
    "y_raw = y_raw[mask_frec]\n",
    "\n",
    "# Codificar etiquetas\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_raw)\n",
    "\n",
    "# Eliminar clases con solo 1 muestra\n",
    "mask = [Counter(y_encoded)[label] > 1 for label in y_encoded]\n",
    "X = X[mask]\n",
    "y_encoded = y_encoded[mask]\n",
    "\n",
    "# 5Ô∏è‚É£ Dividir conjunto de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# 6Ô∏è‚É£ Escalado\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 7Ô∏è‚É£ Definir modelo\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 128, 64),\n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    alpha=0.001,\n",
    "                    batch_size=32,\n",
    "                    learning_rate='adaptive',\n",
    "                    max_iter=100,\n",
    "                    early_stopping=True,\n",
    "                    validation_fraction=0.1,\n",
    "                    n_iter_no_change=10,\n",
    "                    random_state=42,\n",
    "                    verbose=True)\n",
    "\n",
    "# 8Ô∏è‚É£ Entrenar modelo\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# 9Ô∏è‚É£ Evaluaci√≥n\n",
    "y_pred = mlp.predict(X_test)\n",
    "y_pred_proba = mlp.predict_proba(X_test)\n",
    "\n",
    "print(\"\\nüìã Reporte de clasificaci√≥n:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# # üîØ Matriz de confusi√≥n\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "#             xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "# plt.title(\"Matriz de Confusi√≥n - MLPClassifier\")\n",
    "# plt.xlabel(\"Predicci√≥n\")\n",
    "# plt.ylabel(\"Etiqueta Real\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # üîØ Curvas ROC por clase\n",
    "# y_test_bin = label_binarize(y_test, classes=np.arange(len(le.classes_)))\n",
    "# plt.figure(figsize=(8,6))\n",
    "# for i in range(len(le.classes_)):\n",
    "#     fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     plt.plot(fpr, tpr, label=f'{le.classes_[i]} (AUC = {roc_auc:.2f})')\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.title('Curvas ROC por clase - MLPClassifier')\n",
    "# plt.xlabel('Tasa de Falsos Positivos')\n",
    "# plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c072f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1398598700.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mgit remote add origin https://github.com/julianag18/FinalInfoMedica.git\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git remote add origin https://github.com/julianag18/FinalInfoMedica.git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
